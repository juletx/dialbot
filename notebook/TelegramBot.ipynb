{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TelegramBot.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cG8tH4YcSMap","executionInfo":{"status":"ok","timestamp":1619818459714,"user_tz":-120,"elapsed":1622,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"41d82ccc-08d2-40c7-8d8b-f500effa8121"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxyVJIpGCpup","executionInfo":{"status":"ok","timestamp":1619818459715,"user_tz":-120,"elapsed":1616,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"07604fd8-da75-4f77-f390-d0d6181dd400"},"source":["%cd \"/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot/notebook\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot/notebook'\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zu9bpmbLSUI2","executionInfo":{"status":"ok","timestamp":1619818459716,"user_tz":-120,"elapsed":1612,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"71538d7b-2196-4b7c-d251-0fe9411b2081"},"source":["%cd \"/content/drive/MyDrive/dialbot/notebook\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/13R7fsJaBA2ra3u5WI5-hbHRUYUzCn976/dialbot/notebook\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iuwTfhYNb71","executionInfo":{"status":"ok","timestamp":1619818462899,"user_tz":-120,"elapsed":4788,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"9bae8431-f59f-4a0a-830e-55436a7fc91e"},"source":["pip install python-telegram-bot"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.7/dist-packages (13.5)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (2020.12.5)\n","Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (3.6.3)\n","Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (2018.9)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (5.1.1)\n","Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (56.0.0)\n","Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (1.5.1)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UlYR_2YFk2mW"},"source":["save_path='token.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdYO0XSEkr6N"},"source":["with open(save_path) as creds:\n","    for i, line in enumerate(creds):\n","        if i == 1:\n","            token = line.replace(\"token=\", \"\").replace(\"\\n\", \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPQbW9JCCmhU"},"source":["import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,\n","                src: Tensor) -> Tuple[Tensor]:\n","\n","        embedded = self.dropout(self.embedding(src))\n","\n","        outputs, hidden = self.rnn(embedded)\n","\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrlmOMpTCosz"},"source":["class Attention(nn.Module):\n","    def __init__(self,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tensor:\n","\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","\n","        return F.softmax(attention, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbLkV14gCsP0"},"source":["class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: int,\n","                 attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,\n","                              decoder_hidden: Tensor,\n","                              encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep, a\n","\n","\n","    def forward(self,\n","                input: Tensor,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        weighted_encoder_rep, a = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0), a.squeeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_-VQxpZzSao"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder: nn.Module,\n","                 decoder: nn.Module,\n","                 device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","        for t in range(1, max_len):\n","            output, hidden, _ = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJ35JZRnTWU2","executionInfo":{"status":"ok","timestamp":1619818466984,"user_tz":-120,"elapsed":8847,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"799e5cdf-ec73-4597-d62b-897da2364bd2"},"source":["!pip install tokenizers\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","def train_tokenizer(input_path, output_path, vocab_size=10000):\n","    tokenizer = ByteLevelBPETokenizer()\n","    tokenizer.train(files=[input_path], vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"<s>\", \"</s>\", \"<unk>\"])\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    tokenizer.save_model(output_path)\n","    return tokenizer\n","\n","def get_tokenizer(path):\n","    tokenizer = ByteLevelBPETokenizer(path + 'vocab.json', path + 'merges.txt')\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    return tokenizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfMwC8wCnaUn","executionInfo":{"status":"ok","timestamp":1619818467758,"user_tz":-120,"elapsed":9615,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"667a3689-f739-47b5-80d4-88c230b9690d"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","def clean_line(s):\n","    s = s.lower()\n","    s = re.sub(r\"\\.{3}\", r\".\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def tokenize_line(line):\n","    tokens = word_tokenize(line)\n","    tokens_text = ' '.join(tokens)\n","    return tokens_text"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YZdQN1EuUiO6"},"source":["import torch\n","import random\n","from argparse import ArgumentParser\n","\n","def decode(logits, decoding_strategy='max', k=3, temp=0.4):\n","    tokenizer.decode(logits.topk(10)[1][0].numpy())\n","    if decoding_strategy=='top1':\n","        target = logits.max(1)[1]\n","    elif decoding_strategy=='topk':\n","        target = logits.topk(k)[1][0][random.randint(0, k-1)].unsqueeze(-1)\n","    else:\n","        target = torch.multinomial(logits.squeeze().div(temp).exp().cpu(), 1)\n","    return target\n","\n","def evaluate(sentence, model, decoding_strategy):\n","    sentence = clean_line(sentence)\n","    sentence = tokenize_line(sentence)\n","    with torch.no_grad():\n","        target = torch.Tensor([tokenizer.token_to_id('<s>')]).long()\n","        output_sentence = []\n","        encoder_outputs, hidden = model.encoder(torch.Tensor(tokenizer.encode(sentence).ids).long().unsqueeze(-1))\n","        # attentions = torch.zeros(MAX_LENGTH, 1, len(tokenizer.encode(sentence).ids)).to(device)\n","        for t in range(MAX_LENGTH):\n","            # first input to the decoder is the <sos> token\n","            output, hidden, attention = model.decoder(target, hidden, encoder_outputs)\n","            # attentions[i] = attention\n","            target = decode(output, decoding_strategy)\n","            if target.numpy() == tokenizer.token_to_id('</s>'):\n","                return tokenizer.decode(output_sentence)\n","            else:\n","                output_sentence.append(target.numpy()[0])\n","    return tokenizer.decode(output_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvijPasdZze-"},"source":["MAX_LENGTH = 30\n","INPUT_DIM = 10000\n","OUTPUT_DIM = 10000\n","ENC_EMB_DIM = 512\n","DEC_EMB_DIM = 512\n","ENC_HID_DIM = 1024\n","DEC_HID_DIM = 1024\n","ATTN_DIM = 1024\n","ENC_DROPOUT = 0.2\n","DEC_DROPOUT = 0.2\n","#Load model\n","device = 'cpu'\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","model_en = Seq2Seq(enc, dec, device).to(device)\n","model_en.load_state_dict(torch.load('../model/en/model.pt', map_location=device))\n","tokenizer = get_tokenizer('../model/en/')\n","tokenizer_en = get_tokenizer('../model/en/')\n","model_en.eval()\n","decoding_strategy_en = \"multinomial\"\n","\n","INPUT_DIM1 = 10000\n","OUTPUT_DIM1 = 10000\n","ENC_EMB_DIM1 = 256\n","DEC_EMB_DIM1 = 256\n","ENC_HID_DIM1 = 512\n","DEC_HID_DIM1 = 512\n","ATTN_DIM1 = 64\n","ENC_DROPOUT1 = 0.5\n","DEC_DROPOUT1 = 0.5\n","#Load model\n","device = 'cpu'\n","enc = Encoder(INPUT_DIM1, ENC_EMB_DIM1, ENC_HID_DIM1, DEC_HID_DIM1, ENC_DROPOUT1)\n","attn = Attention(ENC_HID_DIM1, DEC_HID_DIM1, ATTN_DIM1)\n","dec = Decoder(OUTPUT_DIM1, DEC_EMB_DIM1, ENC_HID_DIM1, DEC_HID_DIM1, DEC_DROPOUT1, attn)\n","model_eu = Seq2Seq(enc, dec, device).to(device)\n","model_eu.load_state_dict(torch.load('../model/eu/model.pt', map_location=device))\n","tokenizer_eu = get_tokenizer('../model/eu/')\n","model_eu.eval()\n","decoding_strategy_eu = \"topk\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Qot1kYVipVk"},"source":["from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSssmU4B2RvV"},"source":["/en - Change language to english\n","/eu - Change language to euskera\n","/setdecode - Choose a decoding strategy\n","/help - Show list of commands"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"TZxcuEq1MiiT","executionInfo":{"status":"error","timestamp":1619821336279,"user_tz":-120,"elapsed":781,"user":{"displayName":"Aitor Zubillaga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh38XZ32IvQ0Ma6HNMA2XtPZ_PiulROJrKdZHzejA=s64","userId":"01994148971300120890"}},"outputId":"9e608ed0-2823-404f-b1db-6556d41f2be3"},"source":["import logging\n","\n","from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n","#language\n","lang='en'\n","# Enable logging\n","logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n","                    level=logging.INFO)\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","# Define a few command handlers. These usually take the two arguments update and\n","# context. Error handlers also receive the raised TelegramError object in error.\n","def start(update, context):\n","    \"\"\"Send a message when the command /start is issued.\"\"\"\n","    if lang == 'eu':\n","        update.message.reply_text('Kaixo! Dialbot naiz. Komandoen zerrenda ikusteko /help erabili🧐.')\n","    else:\n","        update.message.reply_text('Hi! I\\'m dialbot. Use /help to see a list of all comands🧐.')\n","\n","def help(update, context):\n","    \"\"\"Send a message when the command /help is issued.\"\"\"\n","    if lang == 'eu':\n","        update.message.reply_text('*Hizkuntza* \\n \\n /en \\- hizkuntza ingelesera aldatu \\n /eu \\- hizkuntza euskerara aldatu \\n \\n *Ezarpenak* \\n \\n /setdecode \\- Aukeratu dekodeketa estrategia', parse_mode='MarkdownV2')\n","    else:\n","        update.message.reply_text('*Language* \\n \\n /en \\- change language to English \\n /eu \\- change language to Euskera \\n \\n *Settings* \\n \\n /setdecode \\- Choose a decoding strategy', parse_mode='MarkdownV2')\n","\n","def eu(update, context):\n","    \"\"\"Send a message when the command /help is issued.\"\"\"\n","    global lang\n","    lang = 'eu'\n","    start(update,context)\n","\n","def en(update, context):\n","    \"\"\"Send a message when the command /help is issued.\"\"\"\n","    global lang\n","    lang = 'en'\n","    start(update, context)\n"," \n","def change(update,context):\n","    bot = context.bot\n","    list_of_strategies = ['top1','topk','multinomial']\n","    button_list = []\n","    if lang == 'eu':\n","        senc='Aukeratu deskodeketa estrategia'\n","    else:\n","        senc='Choose a decoding strategy'\n","    for each in list_of_strategies:\n","        button_list.append(InlineKeyboardButton(each, callback_data = each))\n","    reply_markup=InlineKeyboardMarkup(build_menu(button_list,n_cols=1)) #n_cols = 1 is for single column and mutliple rows\n","    bot.send_message(chat_id=update.message.chat_id, text=senc,reply_markup=reply_markup)\n","\n","\n","def build_menu(buttons,n_cols,header_buttons=None,footer_buttons=None):\n","    menu = [buttons[i:i + n_cols] for i in range(0, len(buttons), n_cols)]\n","    if header_buttons:\n","        menu.insert(0, header_buttons)\n","    if footer_buttons:\n","        menu.append(footer_buttons)\n","    return menu\n","\n","def echo(update, context):\n","    \"\"\"Echo the user message.\"\"\"\n","    global model_en\n","    global model_eu\n","    global tokenizer_en\n","    global tokenizer_eu\n","    global tokenizer\n","    global decoding_strategy_en\n","    global decoding_strategy_eu\n","    user = update.message.text\n","    print(tokenizer_eu)\n","    if lang == 'eu':\n","        tokenizer = tokenizer_eu\n","        sentence = evaluate(user,model_eu, decoding_strategy_eu)\n","    else:\n","        tokenizer = tokenizer_en\n","        sentence = evaluate(user,model_en, decoding_strategy_en)\n","    update.message.reply_text(sentence.capitalize())    \n","\n","def error(update, context):\n","    \"\"\"Log Errors caused by Updates.\"\"\"\n","    logger.warning('Update \"%s\" caused error \"%s\"', update, context.error)\n","\n","\n","def main():\n","    \"\"\"Start the bot.\"\"\"\n","    # Create the Updater and pass it your bot's token.\n","    # Make sure to set use_context=True to use the new context based callbacks\n","    # Post version 12 this will no longer be necessary\n","    updater = Updater(token, use_context=True)\n","\n","    # Get the dispatcher to register handlers\n","    dp = updater.dispatcher\n","\n","    # on different commands - answer in Telegram\n","    dp.add_handler(CommandHandler(\"start\", start))\n","    dp.add_handler(CommandHandler(\"help\", help))\n","    dp.add_handler(CommandHandler(\"eu\", eu))\n","    dp.add_handler(CommandHandler(\"en\", en))\n","    dp.add_handler(CommandHandler('change', change, pass_args=True))\n","    # on noncommand i.e message - echo the message on Telegram\n","    dp.add_handler(MessageHandler(Filters.text, echo))\n","\n","    # log all errors\n","    dp.add_error_handler(error)\n","\n","    # Start the Bot\n","    updater.start_polling()\n","\n","    # Run the bot until you press Ctrl-C or the process receives SIGINT,\n","    # SIGTERM or SIGABRT. This should be used most of the time, since\n","    # start_polling() is non-blocking and will stop the bot gracefully.\n","    updater.idle()\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-88258796a307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-88258796a307>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Make sure to set use_context=True to use the new context based callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Post version 12 this will no longer be necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mupdater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpdater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Get the dispatcher to register handlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"]}]}]}