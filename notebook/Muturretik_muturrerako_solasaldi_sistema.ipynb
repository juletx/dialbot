{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Muturretik_muturrerako_solasaldi_sistema.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wji0RsLkzSaA"},"source":["\n","# Muturretik muturrerako solasaldi sistema\n","\n","## Proposatzailea: Jon Ander Campos\n","\n","## Egileak: Julen Etxaniz eta Aitor Zubillaga\n","\n","## Deskribapena\n","\n","Proiektu honetan ikasketa sakonean oinarritutako muturretik muturrerako solasaldi sistema bat garatuko duzu Bahdanau et al. (2014) [1] lanean oinarritua eta filmetako azpitituluak erabiliz (Lison et al. (2016)) [2]. Honetarako, dialogoa itzulpen ataza bat bezala proposatuko dugu, ikusi hurrengo adibidea:\n","\n","* Itzulpen automatikoa :\n","    * Sistemaren sarrera -> esaldia jatorri hizkuntza batean: \"Egun on guztioi.\"\n","    * Sistemaren irteera -> esaldia helburu hizkuntzan: \"Buenos días a todos.\"\n","* Dialogoa:\n","    * Sistemaren sarrera ->dialogoko partaide baten esaldia: \"Egun on guztioi.\"\n","    * Sistemaren irteera -> sarrerako esaldiari erantzuna: \"Baita zuri ere.\"\n","\n","## Helburuak\n","\n","Helburuak zailtasun mailaren araberakoak izango dira:\n","\n","* Z1: Deskargatu ingeleserako entrenatua izan den muturretik muturrerako\n","solasaldi sistema eta probatu ezazu (inferentzia garaian CPUan exekutatzeko gai izan beharko zinateke). Aztertu itzazu ere sistemaren arkitektura eta\n","entrenamendurako erabili diren datuak.\n","\n","* Z2: Orain duzun sistemak ingelesez bakarrik ulertzen du, zergatik ez hau\n","euskarara moldatu? Deskargatu itzazu euskarazko filmetako azpitituluak eta\n","entrenatu ezazu sistema berri bat. Sistemaren entrenamendua Google\n","Colaboratory erabiliz egin behar baduzu sarearen tamaina txikitu beharko duzu. Kodean bertan topatuko dituzu parametro gomendagarrienak.\n","\n","* Z3 (1. aukera): Esku artean dituzun sistemekin solasteko modu oso interesgarria eskaintzen du Telegramek. Aukeratu ezazu bi sistemetako bat eta moldatu Telegrameko bot bezala funtziona dezan. Kasu honetan sistema inferentziarako erabili behar denez, CPUan exekutatzeko gai izan beharko zinateke.\n","\n","* Z3 (2.aukera): Orain arte erabili dituzun sistema guztiak testuingurua kontutan hartu gabe funtzionatzen dute eta hau oso hurbilpen kaxkarra da dialogorako. Hortaz, saiatu zaitez dialogoaren testuingurua kontuan hartzen duen sistema berri bat garatzen. Nahi bezain besteko konplexutasuna gehitu daiteke atal honetan baina aurreko txandako galdera kontutan hartzea nahikoa izango litzateke testuinguruaren ezagutza minimo bat sistemari emateko.\n","\n","## Materialak\n","\n","Proiektu honetarako materialak hurrengoak dira:\n","\n","* Z1: [Bertan](https://drive.google.com/drive/folders/1a6JIZ96fupi8gHxYf5ytgkBc9zenJtQU) topatuko dituzu sistema eta hau exekutatzeko kodea. Ezertan hasi aurretik irakurri ezazu \"IRAKURRI.txt\" fitxategia.\n","\n","* Z2: Hurrengo helbidean hizkuntza askotako azpititulu fitxategiak dituzu:\n","[http://opus.nlpl.eu/OpenSubtitles-v2018.php​](http://opus.nlpl.eu/OpenSubtitles-v2018.php​). Euskarakoak [​bertan​](http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.eu.gz) daude.\n","\n","* Z3 (1.aukera): Hurrengo helbidean topatuko duzu Telegrameko APIa Pythonerako: https://github.com/python-telegram-bot/python-telegram-bot​. Lehen bota sortzeko jarrabideak [​bertan​](https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions-%E2%80%93-Your-first-Bot) topa ditzakezu.\n","\n","## Erreferentziak\n","\n","Notebook hau hurrengo artikulu eta kodeetan oinarritzen da. Alde batetik, Pytorch-eko tutorialak erabili dira. Bestetik, [Ben Trevett](https://github.com/bentrevett)-en tutorialak ere erabilgarriak izan dira.\n","\n","### Papers\n","\n","[1] Bahdanau, D., Cho, K., & Bengio, Y. (2014). [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473). _arXiv preprint arXiv: 1409. 0473_\n","\n","[2] Lison, P., & Tiedemann, J. (2016). [Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles](https://www.aclweb.org/anthology/L16-1147/).\n","\n","### PyTorch\n","\n","[Language Translation with TorchText](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n","\n","[NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","\n","### GitHub\n","\n","[1 - Sequence to Sequence Learning with Neural Networks.ipynb](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n","\n","[3 - Neural Machine Translation by Jointly Learning to Align and Translate.ipynb](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb)\n","\n","[4 - Packed Padded Sequences, Masking, Inference and BLEU.ipynb](https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb)\n","\n","### Colab\n","\n","[1 - Sequence to Sequence Learning with Neural Networks.ipynb](https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n","\n","[3 - Neural Machine Translation by Jointly Learning to Align and Translate.ipynb](https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb#scrollTo=YzSfu75AtQ97)\n","\n","[4 - Packed Padded Sequences, Masking, Inference and BLEU.ipynb](https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"BTMqafkmUcCu"},"source":["## Get Data\n","\n","Esan bezala, filmetako azpitituluak erabiliko ditugu gure ereduak entrenatzeko. Hurrengo helbidean hizkuntza askotako azpititulu fitxategiak dituzu:\n","[http://opus.nlpl.eu/OpenSubtitles-v2018.php​](http://opus.nlpl.eu/OpenSubtitles-v2018.php​). Euskarakoak [​bertan​](http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.eu.gz) daude."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bs_M9F_AzhOc","executionInfo":{"status":"ok","timestamp":1619784955452,"user_tz":-120,"elapsed":25520,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7fd1f47d-44cd-4ca5-8a33-6e78180bad50"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2EIe7UerL4y","executionInfo":{"status":"ok","timestamp":1619784958945,"user_tz":-120,"elapsed":9512,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"6f982256-aefb-4f44-bd6f-7288c7292eec"},"source":["%cd \"/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot/notebook\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot/notebook\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9a_Wc4c7U36s"},"source":["filename = '../data/eu.txt.gz'\n","!wget -O {filename} http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.eu.gz\n","!gunzip {filename}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLJduNBrvFgZ"},"source":["## Preprocess Text\n","\n","Sarea entrenatu ahal izateko euskarazko dataseta ingelesekoaren formatu berdinean jarri beharko duzu edo kodea moldatu, beraz, aztertu ezazu `en_train.tsv` fitxategia. Komeni da fitxategia garbitzea, adibidez, dialogo esaldi hasieran `-` ikurra ez dugu nahi.\n","\n","Hasteko, `eu.txt` fitxategiko `lines` lerro irakurriko ditugu. Ondoren, lerro bakoitza garbituko dugu, letrak eta `.?!` puntuazio ikurrak bakarrik utziz. Gero, testua tokenizatuko dugu, esaldiko tokenak espazio batekin banatuz. Jarraian, sarrera eta irteera pareak osatuko ditugu `\\t` ikurrarekin banatuz. Amaitzeko, `eu.tsv` fitxategiak gordeko dugu dialogoa."]},{"cell_type":"code","metadata":{"id":"db1Sm5El11Nn"},"source":["def get_text(filename, lines=None):\n","    with open(filename) as f:\n","        if lines is None:\n","            text = f.readlines()\n","        else:\n","            text = f.readlines()[:lines+1]\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OW4c1h3O78fW","executionInfo":{"status":"ok","timestamp":1619784971880,"user_tz":-120,"elapsed":1086,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import re\n","def clean_line(s):\n","    s = s.lower()\n","    s = re.sub(r\"\\.{3}\", r\".\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def clean_text(lines):\n","    clean = []\n","    for line in lines:\n","        clean.append(clean_line(line))\n","    return clean"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L86HqbYvCw0b","executionInfo":{"status":"ok","timestamp":1619784976475,"user_tz":-120,"elapsed":3720,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"366991b3-bed5-437c-d0c8-7a8a761ba90c"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","\n","def tokenize_line(line):\n","    tokens = word_tokenize(line)\n","    tokens_text = ' '.join(tokens)\n","    return tokens_text\n","\n","def tokenize_text(text):\n","    tokenized_text = []\n","    for line in tqdm(text, position=0, leave=True):\n","        if line != \" \":\n","            tokens_text = tokenize_line(line)\n","            tokenized_text.append(tokens_text)\n","    return tokenized_text"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_VjWhkj53c0F"},"source":["def get_dialogs(text):\n","    dialogs = []\n","    for i, line in enumerate(text[1:]):\n","        dialog = text[i] + \"\\t\" + text[i+1]\n","        dialogs.append(dialog)\n","    return dialogs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hD3Nr0UGWba"},"source":["def save_text(filename, lines):\n","    with open(filename, 'w') as f:\n","        for line in lines:\n","            f.write(line + \"\\n\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqnAKZAU2Z6V","executionInfo":{"status":"ok","timestamp":1619640799607,"user_tz":-120,"elapsed":586,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"1771eef0-a31d-436e-8fd3-fb711aa593a9"},"source":["text = get_text(\"../data/eu.txt\", lines=500000)\n","text[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hauxe idatzita dago...\\n',\n"," 'Hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean:\\n',\n"," '...naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean.\\n',\n"," 'Hau ere esana dago harrezkero:\\n',\n"," '\"Antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak, alegia.\\n']"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmuUtw913CBx","executionInfo":{"status":"ok","timestamp":1619640803041,"user_tz":-120,"elapsed":2387,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"464f99bd-fea6-4d82-d164-9ce6db1b5176"},"source":["text = clean_text(text)\n","text[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago. ',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean ',\n"," '.naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean. ',\n"," 'hau ere esana dago harrezkero ',\n"," ' antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak alegia. ']"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tzz6f94uDEYm","executionInfo":{"status":"ok","timestamp":1619640845467,"user_tz":-120,"elapsed":41686,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"20407fe8-545a-4b36-9336-c35d5b945a3d"},"source":["text = tokenize_text(text)\n","text[:5]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 500001/500001 [00:41<00:00, 12140.66it/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago .',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean',\n"," '.naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .',\n"," 'hau ere esana dago harrezkero',\n"," 'antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak alegia .']"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAqXikyPEdIW","executionInfo":{"status":"ok","timestamp":1619640876976,"user_tz":-120,"elapsed":2088,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"e5d3bb94-c3ad-4589-e206-2f828d43142e"},"source":["text = get_dialogs(text)\n","text[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago .\\thauxe idatzita dago hasierako garaietatik harri zahar hauen gainean',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean\\t.naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .',\n"," '.naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .\\thau ere esana dago harrezkero',\n"," 'hau ere esana dago harrezkero\\tantzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak alegia .',\n"," 'antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak alegia .\\tgizakia beti egon da izaki horien beldurrez .']"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"ylUjvCI3FxbF"},"source":["save_text(\"../data/eu.tsv\", text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQDfyBlnzSaQ"},"source":["## Vocabulary\n","\n","Sortu hiztegia BPE algoritmoa erabiliz (Sennrich et al., 2015) [1].  Defektuz 10000 subtoken definituko ditugu. \n","\n","[1] Sennrich, R., Haddow, B., & Birch, A. (2015). [Neural machine translation of rare words with subword units](https://arxiv.org/abs/1508.07909). arXiv preprint arXiv:1508.07909.\n","\n","`train_tokenizer` funtzioak `eu.tsv` fitxategia erabiliz tokenizatzailea entrenatuko du, `vocab.json` eta `merges.txt` fitxategietan gordez.\n","\n","`get_tokenizer` funtzioak aurreko fitxategiak erabiliz tokenizatzailea eskuratuko du.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"le3gkSEXWobe","executionInfo":{"status":"ok","timestamp":1619784992691,"user_tz":-120,"elapsed":5760,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"4c47cae5-04aa-46c7-c283-f9e40f92dea6"},"source":["!pip install tokenizers\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","def train_tokenizer(input_path, output_path, vocab_size=10000):\n","    tokenizer = ByteLevelBPETokenizer()\n","    tokenizer.train(files=[input_path], vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"<s>\", \"</s>\", \"<unk>\"])\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    tokenizer.save_model(output_path)\n","    return tokenizer\n","\n","def get_tokenizer(path):\n","    tokenizer = ByteLevelBPETokenizer(path + 'vocab.json', path + 'merges.txt')\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    return tokenizer"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 10.9MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.10.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TAPhlc5WzSaT"},"source":["tokenizer_en = train_tokenizer('../data/en_train.tsv', '../model/en/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGTLiG1iMdpl","executionInfo":{"status":"ok","timestamp":1619785015182,"user_tz":-120,"elapsed":23384,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["tokenizer_eu = train_tokenizer('../data/eu.tsv', '../model/eu/')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"dADoLo2xuI7p","executionInfo":{"status":"ok","timestamp":1619790769020,"user_tz":-120,"elapsed":1704,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["tokenizer_en = get_tokenizer('../model/en/')"],"execution_count":168,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2-xPMxmzSab"},"source":["## Dialog Dataset\n","\n","Esan bezala, filmetako azpitituluak erabiliko ditugu gure ereduak entrenatzeko. Hurrengo helbidean hizkuntza askotako azpititulu fitxategiak dituzu:\n","[http://opus.nlpl.eu/OpenSubtitles-v2018.php​](http://opus.nlpl.eu/OpenSubtitles-v2018.php​). Euskarakoak [​bertan​](http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.eu.gz) daude.\n"]},{"cell_type":"code","metadata":{"id":"VeZEfV_mXUoV","executionInfo":{"status":"ok","timestamp":1619785016100,"user_tz":-120,"elapsed":880,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from torch.utils.data import Dataset\n","\n","class DialogDataset(Dataset):\n","\n","    def __init__(self, dataset_path, tokenizer):\n","\n","        self.tokenizer = tokenizer\n","\n","        self.examples = [(self.tokenizer.encode(line.strip().split('\\t')[0]).ids, \\\n","                          self.tokenizer.encode(line.strip().split('\\t')[1]).ids) \\\n","                         for line in open(dataset_path, 'r', encoding='utf-8').readlines()]\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i):\n","        return torch.tensor(self.examples[i][0]), torch.tensor(self.examples[i][1])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_AbwsyxeBYf","executionInfo":{"status":"ok","timestamp":1619785016103,"user_tz":-120,"elapsed":854,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import torch.nn as nn\n","\n","BATCH_SIZE = 64\n","MAX_LENGTH = 50\n","\n","def generate_batch(data_batch):\n","    src_batch, trg_batch = [], []\n","    for example in data_batch:\n","        src_batch.append(example[0][:MAX_LENGTH])\n","        trg_batch.append(example[1][:MAX_LENGTH])\n","    return nn.utils.rnn.pad_sequence(src_batch, tokenizer_eu.token_to_id('[PAD]')), \\\n","        nn.utils.rnn.pad_sequence(trg_batch, tokenizer_eu.token_to_id('[PAD]'))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UR_27TCSHMKC"},"source":["### English"]},{"cell_type":"code","metadata":{"id":"MJrDoOTWH3NU"},"source":["dataset = DialogDataset('../data/en_train.tsv', tokenizer_en)\n","\n","train_sampler = RandomSampler(dataset)\n","train_dataloader_en = DataLoader(dataset, sampler=train_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KP-pkxgYHKC9"},"source":["### Euskara"]},{"cell_type":"code","metadata":{"id":"TKWGCywmHRjo"},"source":["from torch.utils.data import RandomSampler, DataLoader, random_split\n","\n","def get_dataloaders(filename, tokenizer):\n","    dataset = DialogDataset(filename, tokenizer)\n","    \n","    dataset_size = len(dataset)\n","    train_size = int(0.8 * dataset_size)\n","    val_size = int(0.1 * dataset_size)\n","    test_size = dataset_size - train_size - val_size\n","\n","    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","    train_sampler = RandomSampler(train_dataset)\n","    val_sampler = RandomSampler(val_dataset)\n","    test_sampler = RandomSampler(test_dataset)\n","    \n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","\n","    return train_dataloader, val_dataloader, test_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McizbqJEkt2q","executionInfo":{"status":"ok","timestamp":1619785264778,"user_tz":-120,"elapsed":26002,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["train_dataloader, val_dataloader, test_dataloader = get_dataloaders('../data/eu.tsv', tokenizer_eu)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Drq-SOFZpAgx"},"source":["## Model\n","\n","Muturretik muturrerako sistema hau  Neural machine translation by jointly learning to align and translate (Bahdanau, D., Cho, K., & Bengio, Y. (2014)) artikuluan proposatutako sisteman oinarritzen da. Hau itzulpen automatikoko sistema bat da, beraz, dialogoa itzulpen automatikoko ataza bat bezala definitzen ari gara. Aukera hau ez da optimoa sinplifikazio handi bat baita, hala ere, esperimentu interesgarriak egiteko aukera ematen digu.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mAYF5oznJeWb"},"source":["The most common sequence-to-sequence (seq2seq) models are *encoder-decoder* models, which commonly use a *recurrent neural network* (RNN) to *encode* the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a *context vector*. We can think of the context vector as being an abstract representation of the entire input sentence. This vector is then *decoded* by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n","\n","![](https://drive.google.com/uc?id=1h_qmGljdxt_NbQXSs6UugRE9TGCz63Ma)\n","\n","The above image shows an example translation. The input/source sentence, \"guten morgen\", is passed through the embedding layer (yellow) and then input into the encoder (green). We also append a *start of sequence* (`<sos>`) and *end of sequence* (`<eos>`) token to the start and end of sentence, respectively. At each time-step, the input to the encoder RNN is both the embedding, $e$, of the current word, $e(x_t)$, as well as the hidden state from the previous time-step, $h_{t-1}$, and the encoder RNN outputs a new hidden state $h_t$. We can think of the hidden state as a vector representation of the sentence so far. The RNN can be represented as a function of both of $e(x_t)$ and $h_{t-1}$:\n","\n","$$h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$$\n","\n","We're using the term RNN generally here, it could be any recurrent architecture, such as an *LSTM* (Long Short-Term Memory) or a *GRU* (Gated Recurrent Unit). \n","\n","Here, we have $X = \\{x_1, x_2, ..., x_T\\}$, where $x_1 = \\text{<sos>}, x_2 = \\text{guten}$, etc. The initial hidden state, $h_0$, is usually either initialized to zeros or a learned parameter.\n","\n","Once the final word, $x_T$, has been passed into the RNN via the embedding layer, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T = z$. This is a vector representation of the entire source sentence.\n","\n","Now we have our context vector, $z$, we can start decoding it to get the output/target sentence, \"good morning\". Again, we append start and end of sequence tokens to the target sentence. At each time-step, the input to the decoder RNN (blue) is the embedding, $d$, of current word, $d(y_t)$, as well as the hidden state from the previous time-step, $s_{t-1}$, where the initial decoder hidden state, $s_0$, is the context vector, $s_0 = z = h_T$, i.e. the initial decoder hidden state is the final encoder hidden state. Thus, similar to the encoder, we can represent the decoder as:\n","\n","$$s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$$\n","\n","Although the input/source embedding layer, $e$, and the output/target embedding layer, $d$, are both shown in yellow in the diagram they are two different embedding layers with their own parameters.\n","\n","In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a `Linear` layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$. \n","\n","$$\\hat{y}_t = f(s_t)$$\n","\n","The words in the decoder are always generated one after another, with one per time-step. We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called *teacher forcing*, see a bit more info about it [here](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n","\n","When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference it is common to keep generating words until the model outputs an `<eos>` token or after a certain amount of words have been generated.\n","\n","Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_1, y_2, ..., y_T \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model."]},{"cell_type":"markdown","metadata":{"id":"9p-7dccDCcqT"},"source":["### Encoder\n","\n","First, we'll build the encoder. Similar to the previous model, we only use a single layer GRU, however we now use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. \n","\n","![](https://drive.google.com/uc?id=1iGTlpCoRDms_wPB-TIqPNz17fxInI2w0)\n","\n","We now have:\n","\n","$$\\begin{align*}\n","h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n","h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n","\\end{align*}$$\n","\n","Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n","\n","As before, we only pass an input (`embedded`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\n","\n","The RNN returns `outputs` and `hidden`. \n","\n","`outputs` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\n","\n","`hidden` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n","\n","As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and we currently have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \n","\n","$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n","\n","**Note**: this is actually a deviation from the paper. Instead, they feed only the first backward RNN hidden state through a linear layer to get the context vector/decoder initial hidden state. This doesn't seem to make sense to me, so we have changed it.\n","\n","As we want our model to look back over the whole of the source sentence we return `outputs`, the stacked forward and backward hidden states for every token in the source sentence. We also return `hidden`, which acts as our initial hidden state in the decoder."]},{"cell_type":"code","metadata":{"id":"UPQbW9JCCmhU","executionInfo":{"status":"ok","timestamp":1619785264782,"user_tz":-120,"elapsed":20210,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,\n","                src: Tensor) -> Tuple[Tensor]:\n","\n","        embedded = self.dropout(self.embedding(src))\n","\n","        outputs, hidden = self.rnn(embedded)\n","\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","        return outputs, hidden"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXjxBG7CCgLz"},"source":["### Attention\n","\n","Next up is the attention layer. This will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\n","\n","Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$. \n","\n","First, we calculate the *energy* between the previous decoder hidden state and the encoder hidden states. As our encoder hidden states are a sequence of $T$ tensors, and our previous decoder hidden state is a single tensor, the first thing we do is `repeat` the previous decoder hidden state $T$ times. We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (`attn`) and a $\\tanh$ activation function. \n","\n","$$E_t = \\tanh(\\text{attn}(s_{t-1}, H))$$ \n","\n","This can be thought of as calculating how well each encoder hidden state \"matches\" the previous decoder hidden state.\n","\n","We currently have a **[dec hid dim, src len]** tensor for each example in the batch. We want this to be **[src len]** for each example in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the `energy` by a **[1, dec hid dim]** tensor, $v$.\n","\n","$$\\hat{a}_t = v E_t$$\n","\n","We can think of $v$ as the weights for a weighted sum of the energy across all encoder hidden states. These weights tell us how much we should attend to each token in the source sequence. The parameters of $v$ are initialized randomly, but learned with the rest of the model via backpropagation. Note how $v$ is not dependent on time, and the same $v$ is used for each time-step of the decoding. We implement $v$ as a linear layer without a bias.\n","\n","Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n","\n","$$a_t = \\text{softmax}(\\hat{a_t})$$\n","\n","This gives us the attention over the source sentence!\n","\n","Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/teal blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\n","\n","![](https://drive.google.com/uc?id=1i9NfnK9oWygtnovDCngkyl4x0gJWTlD_)"]},{"cell_type":"code","metadata":{"id":"OrlmOMpTCosz","executionInfo":{"status":"ok","timestamp":1619785267395,"user_tz":-120,"elapsed":2555,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["class Attention(nn.Module):\n","    def __init__(self,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tensor:\n","\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","\n","        return F.softmax(attention, dim=1)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4Huh1Y3Cekb"},"source":["### Decoder\n","\n","Next up is the decoder. \n","\n","The decoder contains the attention layer, `attention`, which takes the previous hidden state, $s_{t-1}$, all of the encoder hidden states, $H$, and returns the attention vector, $a_t$.\n","\n","We then use this attention vector to create a weighted source vector, $w_t$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $a_t$ as the weights.\n","\n","$$w_t = a_t H$$\n","\n","The embedded input word, $d(y_t)$, the weighted source vector, $w_t$, and the previous decoder hidden state, $s_{t-1}$, are then all passed into the decoder RNN, with $d(y_t)$ and $w_t$ being concatenated together.\n","\n","$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n","\n","We then pass $d(y_t)$, $w_t$ and $s_t$ through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. This is done by concatenating them all together.\n","\n","$$\\hat{y}_{t+1} = f(d(y_t), w_t, s_t)$$\n","\n","The image below shows decoding the first word in an example translation.\n","\n","![](https://drive.google.com/uc?id=1i19orNhXYqNE-W0goW6_e6Oi7B0MHcmu)\n","\n","The green/teal blocks show the forward/backward encoder RNNs which output $H$, the red block shows the context vector, $z = h_T = \\tanh(g(h^\\rightarrow_T,h^\\leftarrow_T)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$, the blue block shows the decoder RNN which outputs $s_t$, the purple block shows the linear layer, $f$, which outputs $\\hat{y}_{t+1}$ and the orange block shows the calculation of the weighted sum over $H$ by $a_t$ and outputs $w_t$. Not shown is the calculation of $a_t$."]},{"cell_type":"code","metadata":{"id":"pbLkV14gCsP0","executionInfo":{"status":"ok","timestamp":1619785267401,"user_tz":-120,"elapsed":2532,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: int,\n","                 attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,\n","                              decoder_hidden: Tensor,\n","                              encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep, a\n","\n","\n","    def forward(self,\n","                input: Tensor,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        weighted_encoder_rep, a = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0), a.squeeze(1)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ap_Wa_j-Ci0f"},"source":["### Seq2Seq\n","\n","This is the first model where we don't have to have the encoder RNN and decoder RNN have the same hidden dimensions, however the encoder has to be bidirectional. This requirement can be removed by changing all occurences of `enc_dim * 2` to `enc_dim * 2 if encoder_is_bidirectional else enc_dim`.\n","\n","![](https://drive.google.com/uc?id=1h_qmGljdxt_NbQXSs6UugRE9TGCz63Ma)\n","\n","This seq2seq encapsulator is similar to the last two. The only difference is that the `encoder` returns both the final hidden state (which is the final hidden state from both the forward and backward encoder RNNs passed through a linear layer) to be used as the initial hidden state for the decoder, as well as every hidden state (which are the forward and backward hidden states stacked on top of each other). We also need to ensure that `hidden` and `encoder_outputs` are passed to the decoder. \n","\n","Briefly going over all of the steps:\n","- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n","- the source sequence, $X$, is fed into the encoder to receive $z$ and $H$\n","- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n","- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n","- we then decode within a loop:\n","  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and all encoder outputs, $H$, into the decoder\n","  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n","  - we then decide if we are going to teacher force or not, setting the next input as appropriate"]},{"cell_type":"code","metadata":{"id":"X_-VQxpZzSao","executionInfo":{"status":"ok","timestamp":1619785267404,"user_tz":-120,"elapsed":2513,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder: nn.Module,\n","                 decoder: nn.Module,\n","                 device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","        for t in range(1, max_len):\n","            output, hidden, _ = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW1kpqVczSa2"},"source":["## Train\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AbTC-9VhHn4l"},"source":["### English"]},{"cell_type":"markdown","metadata":{"id":"PEGym1b4AfjQ"},"source":["We initialise our parameters, encoder, decoder and seq2seq model."]},{"cell_type":"code","metadata":{"id":"UmAjiclTIVW3"},"source":["INPUT_DIM = 10000\n","OUTPUT_DIM = 10000\n","ENC_EMB_DIM = 512\n","DEC_EMB_DIM = 512\n","ENC_HID_DIM = 1024\n","DEC_HID_DIM = 1024\n","ATTN_DIM = 1024\n","ENC_DROPOUT = 0.2\n","DEC_DROPOUT = 0.2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model_en = Seq2Seq(enc, dec, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5qyjaIII-T2"},"source":["Print model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZN-Ys7kI-T4","executionInfo":{"status":"ok","timestamp":1619791659902,"user_tz":-120,"elapsed":1218,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"575ee7a0-a7de-423a-c5ad-418222b80125"},"source":["print(model_en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(10000, 512)\n","    (rnn): GRU(512, 1024, bidirectional=True)\n","    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=3072, out_features=1024, bias=True)\n","    )\n","    (embedding): Embedding(10000, 512)\n","    (rnn): GRU(2560, 1024)\n","    (out): Linear(in_features=3584, out_features=10000, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_UIwoNCUI-T6"},"source":["\n","Calculate the number of parameters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DdKJpJHI-T7","executionInfo":{"status":"ok","timestamp":1619791661413,"user_tz":-120,"elapsed":611,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"8561f9d6-51be-4a3f-aa40-a04fada2fff9"},"source":["def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model_en):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 71,800,592 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u-ye_MehI-T8"},"source":["We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."]},{"cell_type":"code","metadata":{"id":"VzyIM-ODI-T9"},"source":["def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","model_en.apply(init_weights);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2eTYCumI-T9"},"source":["We create an optimizer."]},{"cell_type":"code","metadata":{"id":"fS-SZztQI-T-"},"source":["optimizer = optim.Adam(model_en.parameters()) # lr=1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-hgmarPI-T_"},"source":["We initialize the loss function."]},{"cell_type":"code","metadata":{"id":"EOF8dcMgI-T_"},"source":["#Ignore the index of the padding\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTNkCLF2HssO"},"source":["### Euskara"]},{"cell_type":"markdown","metadata":{"id":"qM-t58UVJ40E"},"source":["We initialise our parameters, encoder, decoder and seq2seq model."]},{"cell_type":"code","metadata":{"id":"pClkZpQXxKUL","executionInfo":{"status":"ok","timestamp":1619791657659,"user_tz":-120,"elapsed":1509,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ATTN_DIM = 64\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model_eu = Seq2Seq(enc, dec, device).to(device)"],"execution_count":181,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzNHhLnaBUSH"},"source":["Print model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K1FWktuBWWQ","executionInfo":{"status":"ok","timestamp":1619791659902,"user_tz":-120,"elapsed":1218,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"575ee7a0-a7de-423a-c5ad-418222b80125"},"source":["print(model_eu)"],"execution_count":182,"outputs":[{"output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(10000, 512)\n","    (rnn): GRU(512, 1024, bidirectional=True)\n","    (fc): Linear(in_features=2048, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=3072, out_features=1024, bias=True)\n","    )\n","    (embedding): Embedding(10000, 512)\n","    (rnn): GRU(2560, 1024)\n","    (out): Linear(in_features=3584, out_features=10000, bias=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"luCm6qc4_6dG"},"source":["\n","Calculate the number of parameters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CW1Oi5eu9ZaB","executionInfo":{"status":"ok","timestamp":1619791661413,"user_tz":-120,"elapsed":611,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"8561f9d6-51be-4a3f-aa40-a04fada2fff9"},"source":["def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model_eu):,} trainable parameters')"],"execution_count":183,"outputs":[{"output_type":"stream","text":["The model has 71,800,592 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IpmNyxrf_zjG"},"source":["We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."]},{"cell_type":"code","metadata":{"id":"pGX_p4_w9WT4","executionInfo":{"status":"ok","timestamp":1619791663432,"user_tz":-120,"elapsed":988,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","model_eu.apply(init_weights);"],"execution_count":184,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSj2U24PANKT"},"source":["We create an optimizer."]},{"cell_type":"code","metadata":{"id":"ecreAWy_9cFa","executionInfo":{"status":"ok","timestamp":1619791666552,"user_tz":-120,"elapsed":957,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["optimizer = optim.Adam(model_eu.parameters()) # lr=1e-4"],"execution_count":185,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSb4Z32fAPu-"},"source":["We initialize the loss function."]},{"cell_type":"code","metadata":{"id":"n-itG0BeARCt","executionInfo":{"status":"ok","timestamp":1619791672610,"user_tz":-120,"elapsed":1209,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["#Ignore the index of the padding\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_eu.token_to_id('[PAD]'))"],"execution_count":186,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZ5PZteGASVW"},"source":["### Train function"]},{"cell_type":"code","metadata":{"id":"-OUZ5Gz7zSa4","executionInfo":{"status":"ok","timestamp":1619785279759,"user_tz":-120,"elapsed":1491,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import math\n","import time\n","from tqdm import tqdm\n","\n","def train(model: nn.Module,\n","          train_dataloader: DataLoader,\n","          optimizer: optim.Optimizer,\n","          criterion: nn.Module,\n","          clip: float):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","    for iteration, (src, trg) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0, leave=True):\n","\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        if iteration % 1000 == 0 and iteration > 0:\n","            train_loss = epoch_loss / iteration\n","            train_ppl = math.exp(train_loss)\n","            print(f'\\n\\tIteration: {iteration}')\n","            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n","            tb_writer.add_scalar('train_loss', train_loss, iteration / 1000)\n","            tb_writer.add_scalar('train_ppl', train_ppl, iteration / 1000)\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(train_dataloader)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dfZZ5QGBsl4"},"source":["### Validation function"]},{"cell_type":"code","metadata":{"id":"sDd5bmPhBy89","executionInfo":{"status":"ok","timestamp":1619785282237,"user_tz":-120,"elapsed":1030,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def valid(model: nn.Module, \n","             val_dataloader: DataLoader,\n","             criterion: nn.Module):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for iteration, (src, trg) in tqdm(enumerate(val_dataloader), total=len(val_dataloader), position=0, leave=True):\n","\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(val_dataloader)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyEpGT_dB2r0"},"source":["### Train loop"]},{"cell_type":"code","metadata":{"id":"t-isr__2B1oM","executionInfo":{"status":"ok","timestamp":1619785284785,"user_tz":-120,"elapsed":1174,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def epoch_time(start_time: int,\n","               end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"yyh5y2P5vQ7b"},"source":["filepath = '../model/eu/model.pt.tar'\n","checkpoint = torch.load(filepath)\n","model_eu.load_state_dict(checkpoint['model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","epoch = checkpoint['epoch']\n","best_valid_loss = checkpoint['best_valid_loss']\n","train_loss_plot = checkpoint['train_loss_plot']\n","valid_loss_plot = checkpoint['valid_loss_plot']\n","train_ppl_plot = checkpoint['train_ppl_plot']\n","valid_ppl_plot = checkpoint['valid_ppl_plot']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cc32qwlEERv"},"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","N_EPOCHS = 10\n","CLIP = 5.0\n","tb_writer = SummaryWriter('../logs/')\n","\n","def train_model(model, epoch=0, best_valid_loss=float('inf'), train_loss_plot=[], valid_loss_plot=[], train_ppl_plot=[], valid_ppl_plot=[]):\n","    for epoch in tqdm(range(epoch, N_EPOCHS), position=0, leave=True):\n","        start_time = time.time()\n","\n","        train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n","        valid_loss = valid(model, val_dataloader, criterion)\n","\n","        end_time = time.time()\n","\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","        train_loss_plot.append(train_loss)\n","        valid_loss_plot.append(valid_loss)\n","        train_ppl = math.exp(train_loss)\n","        valid_ppl = math.exp(valid_loss)\n","        train_ppl_plot.append(train_ppl)\n","        valid_ppl_plot.append(valid_ppl)\n","\n","        tb_writer.add_scalar('train_loss', train_loss, epoch)\n","        tb_writer.add_scalar('train_ppl', train_ppl, epoch)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","        \n","        torch.save(model.state_dict(), '../model/eu/model.pt')\n","        torch.save({\n","            'epoch': epoch,\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'best_valid_loss': best_valid_loss,\n","            'train_loss_plot': train_loss_plot,\n","            'valid_loss_plot': valid_loss_plot,\n","            'train_ppl_plot': train_ppl_plot,\n","            'valid_ppl_plot': valid_ppl_plot,\n","            }, filepath)\n","\n","        print(f'\\nEpoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n","        print(f'Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f}')\n","\n","    return train_loss_plot, valid_loss_plot, train_ppl_plot, valid_ppl_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WhBgtzsbvSoF"},"source":["train_loss_plot, valid_loss_plot, train_ppl_plot, valid_ppl_plot = \\\n","    train_model(model_eu, epoch, best_valid_loss, train_loss_plot, valid_loss_plot, train_ppl_plot, valid_ppl_plot)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3I8OP43PIRVY","outputId":"2bc99778-ff7d-4905-f879-49afc281eb36"},"source":["train_loss_plot, valid_loss_plot, train_ppl_plot, valid_ppl_plot = train_model(model_eu)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 16%|█▌        | 1002/6248 [02:55<16:11,  5.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 1000\n","\tTrain Loss: 6.230 | Train PPL: 507.939\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 2002/6248 [05:49<11:59,  5.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 2000\n","\tTrain Loss: 6.068 | Train PPL: 431.714\n"],"name":"stdout"},{"output_type":"stream","text":[" 48%|████▊     | 3002/6248 [08:42<09:17,  5.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 3000\n","\tTrain Loss: 5.977 | Train PPL: 394.409\n"],"name":"stdout"},{"output_type":"stream","text":[" 64%|██████▍   | 4002/6248 [11:37<06:18,  5.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 4000\n","\tTrain Loss: 5.913 | Train PPL: 369.857\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 5002/6248 [14:29<03:15,  6.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 5000\n","\tTrain Loss: 5.861 | Train PPL: 351.122\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 6001/6248 [17:24<00:46,  5.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 6000\n","\tTrain Loss: 5.821 | Train PPL: 337.241\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6248/6248 [18:06<00:00,  5.75it/s]\n","100%|██████████| 781/781 [00:48<00:00, 15.99it/s]\n","  0%|          | 1/6248 [00:00<16:06,  6.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 01 | Time: 18m 54s\n","Train Loss: 5.811 | Train PPL: 333.830\n","Valid Loss: 6.222 | Valid PPL: 503.734\n"],"name":"stdout"},{"output_type":"stream","text":[" 16%|█▌        | 1001/6248 [02:51<15:31,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 1000\n","\tTrain Loss: 5.509 | Train PPL: 246.962\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 2002/6248 [05:46<11:44,  6.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 2000\n","\tTrain Loss: 5.506 | Train PPL: 246.216\n"],"name":"stdout"},{"output_type":"stream","text":[" 48%|████▊     | 3002/6248 [08:39<08:46,  6.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 3000\n","\tTrain Loss: 5.491 | Train PPL: 242.475\n"],"name":"stdout"},{"output_type":"stream","text":[" 64%|██████▍   | 4002/6248 [11:35<06:37,  5.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 4000\n","\tTrain Loss: 5.485 | Train PPL: 240.967\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 5002/6248 [14:29<03:58,  5.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 5000\n","\tTrain Loss: 5.477 | Train PPL: 239.063\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 6002/6248 [17:23<00:42,  5.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 6000\n","\tTrain Loss: 5.470 | Train PPL: 237.430\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6248/6248 [18:06<00:00,  5.75it/s]\n","100%|██████████| 781/781 [00:48<00:00, 16.04it/s]\n","  0%|          | 1/6248 [00:00<13:22,  7.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 02 | Time: 18m 54s\n","Train Loss: 5.468 | Train PPL: 237.018\n","Valid Loss: 6.199 | Valid PPL: 492.109\n"],"name":"stdout"},{"output_type":"stream","text":[" 16%|█▌        | 1002/6248 [02:55<16:25,  5.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 1000\n","\tTrain Loss: 5.306 | Train PPL: 201.546\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 2002/6248 [05:49<11:25,  6.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 2000\n","\tTrain Loss: 5.312 | Train PPL: 202.739\n"],"name":"stdout"},{"output_type":"stream","text":[" 48%|████▊     | 3001/6248 [08:43<10:35,  5.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 3000\n","\tTrain Loss: 5.318 | Train PPL: 204.004\n"],"name":"stdout"},{"output_type":"stream","text":[" 64%|██████▍   | 4002/6248 [11:39<06:12,  6.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 4000\n","\tTrain Loss: 5.323 | Train PPL: 204.902\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 5002/6248 [14:31<03:55,  5.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 5000\n","\tTrain Loss: 5.325 | Train PPL: 205.320\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 6002/6248 [17:25<00:40,  6.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 6000\n","\tTrain Loss: 5.324 | Train PPL: 205.169\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6248/6248 [18:08<00:00,  5.74it/s]\n","100%|██████████| 781/781 [00:49<00:00, 15.94it/s]\n","  0%|          | 1/6248 [00:00<18:00,  5.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 03 | Time: 18m 57s\n","Train Loss: 5.325 | Train PPL: 205.454\n","Valid Loss: 6.193 | Valid PPL: 489.082\n"],"name":"stdout"},{"output_type":"stream","text":[" 16%|█▌        | 1001/6248 [02:55<14:50,  5.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 1000\n","\tTrain Loss: 5.189 | Train PPL: 179.268\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 2001/6248 [05:48<11:42,  6.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 2000\n","\tTrain Loss: 5.193 | Train PPL: 180.055\n"],"name":"stdout"},{"output_type":"stream","text":[" 48%|████▊     | 3002/6248 [08:42<09:41,  5.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 3000\n","\tTrain Loss: 5.201 | Train PPL: 181.414\n"],"name":"stdout"},{"output_type":"stream","text":[" 64%|██████▍   | 4002/6248 [11:36<06:21,  5.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 4000\n","\tTrain Loss: 5.205 | Train PPL: 182.181\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 5002/6248 [14:30<03:10,  6.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 5000\n","\tTrain Loss: 5.210 | Train PPL: 183.074\n"],"name":"stdout"},{"output_type":"stream","text":[" 96%|█████████▌| 6001/6248 [17:25<00:50,  4.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 6000\n","\tTrain Loss: 5.217 | Train PPL: 184.345\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6248/6248 [18:07<00:00,  5.74it/s]\n","100%|██████████| 781/781 [00:49<00:00, 15.94it/s]\n","  0%|          | 0/6248 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 04 | Time: 18m 56s\n","Train Loss: 5.219 | Train PPL: 184.659\n","Valid Loss: 6.220 | Valid PPL: 502.525\n"],"name":"stdout"},{"output_type":"stream","text":[" 16%|█▌        | 1002/6248 [02:54<13:35,  6.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 1000\n","\tTrain Loss: 5.068 | Train PPL: 158.868\n"],"name":"stdout"},{"output_type":"stream","text":[" 32%|███▏      | 2002/6248 [05:48<12:41,  5.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 2000\n","\tTrain Loss: 5.078 | Train PPL: 160.449\n"],"name":"stdout"},{"output_type":"stream","text":[" 48%|████▊     | 3002/6248 [08:42<09:12,  5.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 3000\n","\tTrain Loss: 5.096 | Train PPL: 163.361\n"],"name":"stdout"},{"output_type":"stream","text":[" 64%|██████▍   | 4002/6248 [11:36<06:29,  5.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\tIteration: 4000\n","\tTrain Loss: 5.104 | Train PPL: 164.699\n"],"name":"stdout"},{"output_type":"stream","text":[" 68%|██████▊   | 4244/6248 [12:19<06:15,  5.34it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"JxYXqaDBVPgP"},"source":["import matplotlib.pyplot as plt\n","plt.plot(train_loss_plot, label='train_loss')\n","plt.plot(valid_loss_plot, label='valid_loss')\n","plt.legend()\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vulxHTHrVUYR"},"source":["plt.plot(train_ppl_plot, label='train_ppl')\n","plt.plot(valid_ppl_plot, label='valid_ppl')\n","plt.legend()\n","plt.xlabel('Epochs')\n","plt.ylabel('PPL')\n","plt.title('PPL Plot')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHJSKx-OLAqc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619638103558,"user_tz":-120,"elapsed":1114393,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a41cb826-8528-4752-aab3-4a8c75a2dc6e"},"source":["train_loss_plot, valid_loss_plot, train_ppl_plot, valid_ppl_plot = train_model(model_eu)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 625/625 [01:43<00:00,  6.03it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.21it/s]\n","  0%|          | 1/625 [00:00<01:11,  8.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 01 | Time: 1m 48s\n","Train Loss: 6.265 | Train PPL: 525.854\n","Valid Loss: 6.251 | Valid PPL: 518.417\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.90it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.16it/s]\n","  0%|          | 1/625 [00:00<01:35,  6.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 02 | Time: 1m 51s\n","Train Loss: 5.882 | Train PPL: 358.624\n","Valid Loss: 6.196 | Valid PPL: 490.706\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.90it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.16it/s]\n","  0%|          | 1/625 [00:00<01:49,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 03 | Time: 1m 51s\n","Train Loss: 5.710 | Train PPL: 301.920\n","Valid Loss: 6.209 | Valid PPL: 497.253\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:46<00:00,  5.88it/s]\n","100%|██████████| 79/79 [00:05<00:00, 14.82it/s]\n","  0%|          | 0/625 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 04 | Time: 1m 51s\n","Train Loss: 5.552 | Train PPL: 257.658\n","Valid Loss: 6.254 | Valid PPL: 519.834\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.90it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.37it/s]\n","  0%|          | 1/625 [00:00<01:42,  6.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 05 | Time: 1m 51s\n","Train Loss: 5.353 | Train PPL: 211.191\n","Valid Loss: 6.299 | Valid PPL: 543.841\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:46<00:00,  5.88it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.40it/s]\n","  0%|          | 1/625 [00:00<01:14,  8.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 06 | Time: 1m 51s\n","Train Loss: 5.102 | Train PPL: 164.340\n","Valid Loss: 6.429 | Valid PPL: 619.354\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.93it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.15it/s]\n","  0%|          | 1/625 [00:00<01:46,  5.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 07 | Time: 1m 50s\n","Train Loss: 4.751 | Train PPL: 115.690\n","Valid Loss: 6.664 | Valid PPL: 783.395\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.92it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.30it/s]\n","  0%|          | 1/625 [00:00<01:37,  6.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 08 | Time: 1m 50s\n","Train Loss: 4.296 | Train PPL:  73.429\n","Valid Loss: 6.901 | Valid PPL: 993.485\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.90it/s]\n","100%|██████████| 79/79 [00:05<00:00, 14.87it/s]\n","  0%|          | 1/625 [00:00<01:39,  6.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 09 | Time: 1m 51s\n","Train Loss: 3.804 | Train PPL:  44.878\n","Valid Loss: 7.192 | Valid PPL: 1328.838\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 625/625 [01:45<00:00,  5.92it/s]\n","100%|██████████| 79/79 [00:05<00:00, 15.36it/s]\n","100%|██████████| 10/10 [18:33<00:00, 111.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 10 | Time: 1m 50s\n","Train Loss: 3.391 | Train PPL:  29.684\n","Valid Loss: 7.487 | Valid PPL: 1784.809\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QiFJ5BzDQ5DR"},"source":["### Plots"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"I9kEP6iTPmYB","executionInfo":{"status":"ok","timestamp":1619638115360,"user_tz":-120,"elapsed":671,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a48fbbde-aa8d-4257-e345-b3db16fd05ab"},"source":["import matplotlib.pyplot as plt\n","plt.plot(train_loss_plot, label='train_loss')\n","plt.plot(valid_loss_plot, label='valid_loss')\n","plt.legend()\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"e6VOY4RIB0Lr","executionInfo":{"status":"ok","timestamp":1619638118264,"user_tz":-120,"elapsed":916,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7830c307-a704-4078-bc04-62b468f7a0c0"},"source":["plt.plot(train_ppl_plot, label='train_ppl')\n","plt.plot(valid_ppl_plot, label='valid_ppl')\n","plt.legend()\n","plt.xlabel('Epochs')\n","plt.ylabel('PPL')\n","plt.title('PPL Plot')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"AVLkDVNDQ9Q7"},"source":["### TensorBoard\n","\n","https://pytorch.org/docs/stable/tensorboard.html\n","\n","https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n","\n","https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"TgF35qdzIC3T"},"source":["**Authorize TensorBoard.dev**\n","\n","This step requires you to auth in your shell console, outside of Jupyter.  In your console, execute the following command.\n","\n","`tensorboard dev list`\n","\n","As part of this flow, you will be provided with an authorization code. This code is required to consent to the Terms of Service."]},{"cell_type":"code","metadata":{"id":"2Q2LCmL7QxHh"},"source":["!tensorboard dev list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKW8V5chyx6e"},"source":["**Upload to TensorBoard.dev**\n","\n","Uploading the TensorBoard logs will give you a URL that can be shared with anyone.\n","\n","Uploaded TensorBoards are public, so do not upload sensitive data.\n","\n","The uploader will exit when the entire logdir has uploaded.  (This is what the `--one_shot` flag specifies.)"]},{"cell_type":"code","metadata":{"id":"n2PvxhOkW7vn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619377281324,"user_tz":-120,"elapsed":4569,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"be04437b-7b7f-4983-91bd-e0b022b1443a"},"source":["!tensorboard dev upload --logdir ../logs \\\n","  --name \"Dialbot test\" \\\n","  --description \"Training results from https://colab.research.google.com/drive/1XoBBD58GbTBEDC6gf7yULcHe5Q2tMY2G\" \\\n","  --one_shot"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-25 19:01:18.340583: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","Data for the \"text\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n","\n","New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/9qULsmkvS862QKK6SV5Q4g/\n","\n","\u001b[1m[2021-04-25T19:01:20]\u001b[0m Started scanning logdir.\n","\u001b[1m[2021-04-25T19:01:20]\u001b[0m Total uploaded: 6 scalars, 0 tensors, 0 binary objects\n","\u001b[1m[2021-04-25T19:01:20]\u001b[0m Done scanning logdir.\n","\n","\n","Done. View your TensorBoard at https://tensorboard.dev/experiment/9qULsmkvS862QKK6SV5Q4g/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5QH5k4AUNE27"},"source":["Each individual upload has a unique experiment ID. This means that if you start a new upload with the same directory, you will get a new experiment ID.\n","You can list all the experiments you have uploaded using \n","```\n","tensorboard dev list\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CcktvThPBgc","executionInfo":{"status":"ok","timestamp":1619374461023,"user_tz":-120,"elapsed":4915,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"c86a1acb-a8ad-4776-b048-8cd36a7037ef"},"source":["!tensorboard dev list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-04-25 18:14:17.338168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","Data for the \"text\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n","No experiments. Use `tensorboard dev upload` to get started.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7cqbMbjvOSu7"},"source":["## Test"]},{"cell_type":"markdown","metadata":{"id":"4WWNC6kBF-8q"},"source":["### English"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nde_ifP2GGRX","executionInfo":{"status":"ok","timestamp":1619791513649,"user_tz":-120,"elapsed":1366,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"5a436412-7c35-4363-e8b0-b997af119679"},"source":["#Load model\n","devide = 'cuda'\n","model_en.load_state_dict(torch.load('../model/en/model.pt', map_location=device))"],"execution_count":178,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":178}]},{"cell_type":"code","metadata":{"id":"dRhYrxrXF7zM"},"source":["train_loss = valid(model_en, train_dataloader_en, criterion)\n","train_ppl = math.exp(train_loss)\n","print(f'\\nTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lW6vQzEXGDID"},"source":["### Euskara"]},{"cell_type":"code","metadata":{"id":"yz7xGyo3OWY9","executionInfo":{"status":"ok","timestamp":1619789002919,"user_tz":-120,"elapsed":982,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["#Load model\n","devide = 'cuda'\n","model_eu.load_state_dict(torch.load('../model/eu/model.pt', map_location=device))"],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0_O8MCwF4SO","executionInfo":{"status":"ok","timestamp":1619787598966,"user_tz":-120,"elapsed":94439,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"ddbc88fd-f14e-4f2f-e32d-c704dec3f259"},"source":["test_loss = valid(model_eu, test_dataloader, criterion)\n","test_ppl = math.exp(test_loss)\n","print(f'\\nTest Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f}')"],"execution_count":86,"outputs":[{"output_type":"stream","text":["100%|██████████| 781/781 [01:33<00:00,  8.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Test Loss: 5.975 | Test PPL: 393.311\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rv1PowaW2RtO","executionInfo":{"status":"ok","timestamp":1619787694539,"user_tz":-120,"elapsed":95567,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"b6aaab27-72fd-4a35-867a-785cd44e9ced"},"source":["valid_loss = valid(model_eu, val_dataloader, criterion)\n","valid_ppl = math.exp(valid_loss)\n","print(f'\\nTest Loss: {valid_loss:.3f} | Test PPL: {valid_ppl:7.3f}')"],"execution_count":87,"outputs":[{"output_type":"stream","text":["100%|██████████| 781/781 [01:33<00:00,  8.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Test Loss: 5.983 | Test PPL: 396.612\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"p3SoavU_8W5f"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"Qd-OsR4_8fBw","executionInfo":{"status":"ok","timestamp":1619797061866,"user_tz":-120,"elapsed":3130,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import torch\n","import random\n","from argparse import ArgumentParser\n","\n","MAX_LENGTH = 30\n","\n","def decode(logits, tokenizer, decoding_strategy='multinomial', k=3, temp=0.4):\n","    tokenizer.decode(logits.topk(10)[1][0].numpy())\n","    if decoding_strategy=='top1':\n","        target = logits.max(1)[1]\n","    elif decoding_strategy=='topk':\n","        target = logits.topk(k)[1][0][random.randint(0, k-1)].unsqueeze(-1)\n","    else:\n","        target = torch.multinomial(logits.squeeze().div(temp).exp().cpu(), 1)\n","    return target\n","\n","def evaluate(sentence, model, tokenizer, decoding_strategy='multinomial', k=3, temp=0.4):\n","    sentence = clean_line(sentence)\n","    sentence = tokenize_line(sentence)\n","    with torch.no_grad():\n","        target = torch.Tensor([tokenizer.token_to_id('<s>')]).long()\n","        output_sentence = []\n","        encoder_outputs, hidden = model.encoder(torch.Tensor(tokenizer.encode(sentence).ids).long().unsqueeze(-1))\n","        attentions = torch.zeros(MAX_LENGTH, 1, len(tokenizer.encode(sentence).ids)).to(device)\n","        for i in range(MAX_LENGTH):\n","            # first input to the decoder is the <sos> token\n","            output, hidden, attention = model.decoder(target, hidden, encoder_outputs)\n","            attentions[i] = attention\n","            target = decode(output, tokenizer, decoding_strategy, k, temp)\n","            if target.numpy() == tokenizer.token_to_id('</s>'):\n","                return tokenizer.decode(output_sentence)\n","            else:\n","                output_sentence.append(target.numpy()[0])\n","    return tokenizer.decode(output_sentence), attentions"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIgDq_nmRa7B","executionInfo":{"status":"ok","timestamp":1619795257313,"user_tz":-120,"elapsed":489,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def dialog(model, tokenizer, decoding_strategy='multinomial', k=3, temp=0.4):\n","    #Print welcome message\n","    print('-------------------------------')\n","    print('Welcome to Dialbot system')\n","    print(\"Write 'Bye' or 'Agur' to end the system.\")\n","    print('-------------------------------')\n","\n","    # Main system loop\n","    user = input('-')\n","    model.eval()\n","\n","    while user != 'Bye' and user != 'Agur':\n","        sentence = evaluate(user, model, tokenizer, decoding_strategy)\n","        print('+' + sentence.capitalize())\n","        user = input('-')\n","        \n","    sentence = evaluate(user)\n","    print('+' + sentence.capitalize())"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmnSWfMWMSyC"},"source":["### English"]},{"cell_type":"code","metadata":{"id":"VbxrbsArMVEy"},"source":["#Load model\n","device = 'cpu'\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","model_en = Seq2Seq(enc, dec, device).to(device)\n","model_en.load_state_dict(torch.load('../model/en/model.pt', map_location=device))\n","\n","tokenizer_en = get_tokenizer('../model/en/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IsLKDDlZy-2"},"source":["sentence = \"Hello\"\n","output, attention = evaluate(sentence, model_en, tokenizer_en)\n","print(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0MJRsK-SyaC"},"source":["dialog(model_en, tokenizer_en)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a53CxXy_MA3F"},"source":["### Euskara"]},{"cell_type":"code","metadata":{"id":"7lbRHZ1F0H5R"},"source":["#Load model\n","device = 'cpu'\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","model_eu = Seq2Seq(enc, dec, device).to(device)\n","model_eu.load_state_dict(torch.load('../model/eu/model.pt', map_location=device))\n","\n","tokenizer_eu = get_tokenizer('../model/eu/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0op0iaEZ5be"},"source":["sentence = \"Kaixo\"\n","output, attention = evaluate(\"Kaixo\", model_en, tokenizer_en)\n","print(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JT55rcSSo_h"},"source":["dialog(model_eu, tokenizer_eu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJZG2a1bXWZM"},"source":["## Attention\n","\n","Next, we'll make a function that displays the model's attention over the source sentence for each target token generated. Then we'll use our `evaluate` function to get our predicted sentence and attention.  We show this graphically by having the source sentence on the x-axis and the predicted sentence on the y-axis. The lighter the square at the intersection between two words, the more attention the model gave to that source word when translating that target word."]},{"cell_type":"code","metadata":{"id":"dnINkaV0Xd4Y"},"source":["import matplotlib.ticker as ticker\n","def display_attention(sentence, prediction, attention):\n","    sentence = sentence.split(' ')\n","    prediction = prediction.split(' ')\n","    \n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(111)\n","    \n","    attention = attention.squeeze(1).cpu().detach().numpy()\n","    \n","    cax = ax.matshow(attention, cmap='bone')\n","   \n","    ax.tick_params(labelsize=15)\n","    \n","    x_ticks = [''] + ['<s>'] + [t.lower() for t in sentence] + ['<\\s>']\n","    y_ticks = [''] + prediction\n","     \n","    ax.set_xticklabels(x_ticks, rotation=45)\n","    ax.set_yticklabels(y_ticks)\n","\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyUjD_CXaeg2"},"source":["display_attention(sentence, output, attention)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QqNd9ydCRpOf"},"source":["## BLEU\n","\n","Previously we have only cared about the loss/perplexity of the model. However there metrics that are specifically designed for measuring the quality of a translation - the most popular is *BLEU*. Without going into too much detail, BLEU looks at the overlap in the predicted and actual target sequences in terms of their n-grams. It will give us a number between 0 and 1 for each sequence, where 1 means there is perfect overlap, i.e. a perfect translation, although is usually shown between 0 and 100. BLEU was designed for multiple candidate translations per source sequence, however in this dataset we only have one candidate per source.\n","\n","We define a `calculate_bleu` function which calculates the BLEU score over a provided TorchText dataset. This function creates a corpus of the actual and predicted translation for each source sentence and then calculates the BLEU score."]},{"cell_type":"code","metadata":{"id":"cIg72G-6TW_L"},"source":["from torchtext.data.metrics import bleu_score\n","def calculate_bleu(dataloader, model, tokenizer, decoding_strategy='multinomial', k=3, temp=0.4):\n","    targets = []\n","    predictions = []\n","    for iteration, (src, trg) in tqdm(enumerate(dataloader), total=len(dataloader), position=0, leave=True):\n","        src, trg = src.to(device), trg.to(device)\n","\n","        # swap axes and remove <s>\n","        src_swap = src.swapaxes(0, 1)[:, 1:]\n","        trg_swap = trg.swapaxes(0, 1)[:, 1:]\n","\n","        src_sentences = tokenizer.decode_batch(src_swap.numpy())\n","        trg_sentences = tokenizer.decode_batch(trg_swap.numpy())\n","\n","        for i in range(len(src_sentences)):\n","            # remove padding and </s>\n","            target = trg_sentences[i].split(\"</s>\")[0]\n","            target_list = target.split(' ')\n","            targets.append([target_list])\n","\n","            sentence = src_sentences[i].split(\"</s>\")[0]\n","            prediction = evaluate(sentence, model, tokenizer, decoding_strategy, k, temp)\n","            prediction_list = prediction.split(' ')\n","            predictions.append(prediction_list)\n","        \n","        if iteration > 5:\n","            break\n","\n","    return bleu_score(predictions, targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kf8hkXsHDsHw"},"source":["### English"]},{"cell_type":"code","metadata":{"id":"geH862qzD81S"},"source":["#Load model\n","devide = 'cpu'\n","model_en.load_state_dict(torch.load('../model/en/model.pt', map_location=device))\n","tokenizer_en = get_tokenizer('../model/en/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hgWPD51DmiE"},"source":["device = 'cpu'\n","bleu = calculate_bleu(train_dataloader_en, model_en, tokenizer_en)\n","print()\n","print(f'Train BLEU score = {bleu*100:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4N0wGlHDqbO"},"source":["### Euskara"]},{"cell_type":"code","metadata":{"id":"H-XzkFfIROXS"},"source":["#Load model\n","devide = 'cpu'\n","model_eu.load_state_dict(torch.load('../model/eu/model.pt', map_location=device))\n","tokenizer_eu = get_tokenizer('../model/eu/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVbz7UYdCu-D","executionInfo":{"status":"ok","timestamp":1619790669068,"user_tz":-120,"elapsed":21514,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"620a6f76-eb10-4747-b24f-3763784843bf"},"source":["device = 'cpu'\n","bleu = calculate_bleu(train_dataloader, model_eu, tokenizer_eu)\n","print()\n","print(f'Train BLEU score = {bleu*100:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 6/6248 [00:17<5:04:32,  2.93s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","BLEU score = 0.00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGKg3FKSTPm9","executionInfo":{"status":"ok","timestamp":1619790715333,"user_tz":-120,"elapsed":21407,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"f88486ce-efb5-4a98-ba20-4e0d9b10c2b1"},"source":["device = 'cpu'\n","bleu = calculate_bleu(test_dataloader, model_eu, tokenizer_eu)\n","print()\n","print(f'Test BLEU score = {bleu*100:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  1%|          | 6/781 [00:17<38:19,  2.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","BLEU score = 0.00\n"],"name":"stdout"}]}]}