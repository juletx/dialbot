{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Muturretik_muturrerako_solasaldi_sistema.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wji0RsLkzSaA"},"source":["\n","Muturretik muturrerako solasaldi sistema\n","===================================\n","\n","Notebook honetan (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) oinarrituta, muturretik muturrerako solasaldi sistema bat entrenatzeko kodea topatuko duzu. \n","\n","Muturretik muturrerako sistema hau Neural machine translation by jointly learning to align and translate (Bahdanau, D., Cho, K., & Bengio, Y. (2014)) artikuluan proposatutako sisteman oinarritzen da eta solsaldia itzulpen automatikoko ataza bat bezala proposatzen du. \n","\n","Oharra: Notebook hau hurrengo kodean oinarritzen da: https://github.com/spro/practical-pytorch\n"]},{"cell_type":"markdown","metadata":{"id":"ffzoj36pzjCr"},"source":["#PLEASE USE HERE YOUR PATHS IN GOOGLE DRIVE:\n"]},{"cell_type":"code","metadata":{"id":"Bs_M9F_AzhOc"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd \"/content/drive/My Drive/HP laboak/DIAL Proiektua (2020-2021)\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQDfyBlnzSaQ"},"source":["Sortu hiztegia BPE algoritmoa erabiliz (Sennrich et al., 2015).  Defektuz 10000 subtoken definituko ditugu. \n","\n","\n","*Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.(https://arxiv.org/abs/1508.07909). \n","\n"]},{"cell_type":"code","metadata":{"id":"TAPhlc5WzSaT"},"source":["!pip install tokenizers\n","from src.utils import train_tokenizer\n","tokenizer = train_tokenizer('./data/en_train.tsv', './model/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2-xPMxmzSab"},"source":["``DialogDataset``\n","----------------\n","Lehenik gure datuekin dataseta sortuko dugu \n"]},{"cell_type":"code","metadata":{"id":"myseFI2GzSae","executionInfo":{"status":"ok","timestamp":1617100759271,"user_tz":-120,"elapsed":18715,"user":{"displayName":"Jon Ander Campos","photoUrl":"","userId":"11402088787025851681"}}},"source":["from torch.utils.data import RandomSampler, DataLoader\n","from src.dialog_dataset import DialogDataset\n","import torch.nn as nn\n","\n","dataset = DialogDataset('./data/en_train.tsv', tokenizer)\n","BATCH_SIZE = 64\n","MAX_LENGTH = 50\n","\n","#Create train iterator\n","def generate_batch(data_batch):\n","    src_batch, trg_batch = [], []\n","    for example in data_batch:\n","        src_batch.append(example[0][:MAX_LENGTH])\n","        trg_batch.append(example[1][:MAX_LENGTH])\n","    return nn.utils.rnn.pad_sequence(src_batch, tokenizer.token_to_id('[PAD]')), nn.utils.rnn.pad_sequence(trg_batch, tokenizer.token_to_id('[PAD]'))\n","\n","train_sampler = RandomSampler(dataset)\n","train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Drq-SOFZpAgx"},"source":["Sare neuronala definitzen\n","----------------\n","\n","Muturretik muturrerako sistema hau  Neural machine translation by jointly learning to align and translate (Bahdanau, D., Cho, K., & Bengio, Y. (2014)) artikuluan proposatutako sisteman oinarritzen da. Hau itzulpen automatikoko sistema bat da, beraz, dialogoa itzulpen automatikoko ataza bat bezala definitzen ari gara. Aukera hau ez da optimoa sinplifikazio handi bat baita, hala ere, esperimentu interesgarriak egiteko aukera ematen digu.\n","\n"]},{"cell_type":"code","metadata":{"id":"X_-VQxpZzSao"},"source":["import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,\n","                src: Tensor) -> Tuple[Tensor]:\n","\n","        embedded = self.dropout(self.embedding(src))\n","\n","        outputs, hidden = self.rnn(embedded)\n","\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","        return outputs, hidden\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tensor:\n","\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","\n","        return F.softmax(attention, dim=1)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: int,\n","                 attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,\n","                              decoder_hidden: Tensor,\n","                              encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep\n","\n","\n","    def forward(self,\n","                input: Tensor,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0)\n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder: nn.Module,\n","                 decoder: nn.Module,\n","                 device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","        for t in range(1, max_len):\n","            output, hidden = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs\n","\n","#Tamainak egokitu zuen beharretara\n","INPUT_DIM = 10000\n","OUTPUT_DIM = 10000\n","ENC_EMB_DIM = 512\n","DEC_EMB_DIM = 512\n","ENC_HID_DIM = 1024\n","DEC_HID_DIM = 1024\n","ATTN_DIM = 1024\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","\n","def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","\n","model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","\n","def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZY1VBpMxzSaw","executionInfo":{"status":"ok","timestamp":1617100762627,"user_tz":-120,"elapsed":22061,"user":{"displayName":"Jon Ander Campos","photoUrl":"","userId":"11402088787025851681"}}},"source":["#Ignore the index of the padding\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW1kpqVczSa2"},"source":["Sistema entrenatu:\n","\n"]},{"cell_type":"code","metadata":{"id":"-OUZ5Gz7zSa4"},"source":["import math\n","import time\n","from tqdm import tqdm\n","\n","def train(model: nn.Module,\n","          train_dataloader: DataLoader,\n","          optimizer: optim.Optimizer,\n","          criterion: nn.Module,\n","          clip: float):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","    for iteration, (src, trg) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n","\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","\n","def epoch_time(start_time: int,\n","               end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\n","N_EPOCHS = 10\n","CLIP = 5.0\n","\n","best_valid_loss = float('inf')\n","for epoch in tqdm(range(N_EPOCHS), ):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    torch.save(model.state_dict(), './model/model.pt')"],"execution_count":null,"outputs":[]}]}