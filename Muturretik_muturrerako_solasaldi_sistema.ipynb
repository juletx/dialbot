{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Muturretik_muturrerako_solasaldi_sistema.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wji0RsLkzSaA"},"source":["\n","Muturretik muturrerako solasaldi sistema\n","===================================\n","\n","Notebook honetan (https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) oinarrituta, muturretik muturrerako solasaldi sistema bat entrenatzeko kodea topatuko duzu. \n","\n","Muturretik muturrerako sistema hau Neural machine translation by jointly learning to align and translate (Bahdanau, D., Cho, K., & Bengio, Y. (2014)) artikuluan proposatutako sisteman oinarritzen da eta solsaldia itzulpen automatikoko ataza bat bezala proposatzen du. \n","\n","Oharra: Notebook hau hurrengo kodean oinarritzen da: https://github.com/spro/practical-pytorch\n"]},{"cell_type":"code","metadata":{"id":"Bs_M9F_AzhOc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617304407994,"user_tz":-120,"elapsed":1733,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"05ba5ff6-0938-4ff0-8744-00a83c34d505"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2EIe7UerL4y","executionInfo":{"status":"ok","timestamp":1617304407997,"user_tz":-120,"elapsed":1713,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"b4526a68-4fbc-4338-b11c-afb6970e7b32"},"source":["%cd \"/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/2. Lauhilekoa/HP/Lana/dialbot\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-ewxUZb-xYad"},"source":["https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n","\n","https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb"]},{"cell_type":"markdown","metadata":{"id":"iLJduNBrvFgZ"},"source":["## Preprocess Text"]},{"cell_type":"code","metadata":{"id":"db1Sm5El11Nn","executionInfo":{"status":"ok","timestamp":1617304408000,"user_tz":-120,"elapsed":1689,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def get_text(filename, lines):\n","    with open(filename) as f:\n","        text = f.readlines()[:lines+1]\n","    return text"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"OW4c1h3O78fW","executionInfo":{"status":"ok","timestamp":1617304408003,"user_tz":-120,"elapsed":1682,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def clean_text(lines):\n","    clean = []\n","    for line in lines:\n","        clean.append(clean_line(line))\n","    return clean"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HULx0wnz-Lt","executionInfo":{"status":"ok","timestamp":1617304408005,"user_tz":-120,"elapsed":1676,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def clean_line(t):\n","    t = t.replace('-', '')\n","    t = t.replace('\\\"', '')\n","    t = t.replace('\\'', '')\n","    t = t.lower()\n","    return t"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L86HqbYvCw0b","executionInfo":{"status":"ok","timestamp":1617304409650,"user_tz":-120,"elapsed":3311,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7d5316cf-c9c6-4292-ef2b-f17adae15562"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","def tokenize_text(text):\n","    tokenized_text = []\n","    for line in text:\n","        if line != \"\\n\":\n","            tokens = word_tokenize(line)\n","            tokens_text = ' '.join(tokens)\n","            tokenized_text.append(tokens_text)\n","    return tokenized_text"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_VjWhkj53c0F","executionInfo":{"status":"ok","timestamp":1617304409657,"user_tz":-120,"elapsed":3310,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def get_dialogs(text):\n","    dialogs = []\n","    for i, line in enumerate(text[1:]):\n","        dialog = text[i] + \"\\t\" + text[i+1]\n","        dialogs.append(dialog)\n","    return dialogs"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hD3Nr0UGWba","executionInfo":{"status":"ok","timestamp":1617304409659,"user_tz":-120,"elapsed":3301,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def save_text(filename, lines):\n","    with open(filename, 'w') as f:\n","        for line in lines:\n","            f.write(line + \"\\n\")  "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqnAKZAU2Z6V","executionInfo":{"status":"ok","timestamp":1617304411230,"user_tz":-120,"elapsed":4863,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"43132e5d-a44d-4130-f8ee-c19906fce5a9"},"source":["filename = \"./data/eu.txt\"\n","text = get_text(filename, 1019631)\n","text[:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hauxe idatzita dago...\\n',\n"," 'Hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean:\\n',\n"," '...naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean.\\n',\n"," 'Hau ere esana dago harrezkero:\\n',\n"," '\"Antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak, alegia.\\n',\n"," 'Gizakia beti egon da izaki horien beldurrez.\\n',\n"," 'Sorginkeria eta gaizkiaren gurtza existitzen dira gure egunetan ere.\\n',\n"," 'DEABRUAREN GAUA\\n',\n"," 'Hamabost.\\n',\n"," 'Bi gehi hiru berdin bost.\\n']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmuUtw913CBx","executionInfo":{"status":"ok","timestamp":1617304411914,"user_tz":-120,"elapsed":5537,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"93791093-3023-44d9-8f34-cfd6b6809165"},"source":["text = clean_text(text)\n","text[:10]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago...\\n',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean:\\n',\n"," '...naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean.\\n',\n"," 'hau ere esana dago harrezkero:\\n',\n"," 'antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak, alegia.\\n',\n"," 'gizakia beti egon da izaki horien beldurrez.\\n',\n"," 'sorginkeria eta gaizkiaren gurtza existitzen dira gure egunetan ere.\\n',\n"," 'deabruaren gaua\\n',\n"," 'hamabost.\\n',\n"," 'bi gehi hiru berdin bost.\\n']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tzz6f94uDEYm","executionInfo":{"status":"ok","timestamp":1617304492561,"user_tz":-120,"elapsed":86172,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"9b69af8c-255a-4eac-d4d9-5ded0252cf46"},"source":["text = tokenize_text(text)\n","text[:10]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago ...',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean :',\n"," '... naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .',\n"," 'hau ere esana dago harrezkero :',\n"," 'antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak , alegia .',\n"," 'gizakia beti egon da izaki horien beldurrez .',\n"," 'sorginkeria eta gaizkiaren gurtza existitzen dira gure egunetan ere .',\n"," 'deabruaren gaua',\n"," 'hamabost .',\n"," 'bi gehi hiru berdin bost .']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAqXikyPEdIW","executionInfo":{"status":"ok","timestamp":1617304492563,"user_tz":-120,"elapsed":86162,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"20573051-9f0a-455b-a7f2-731a3765bb1a"},"source":["text = get_dialogs(text)\n","text[:10]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hauxe idatzita dago ...\\thauxe idatzita dago hasierako garaietatik harri zahar hauen gainean :',\n"," 'hauxe idatzita dago hasierako garaietatik harri zahar hauen gainean :\\t... naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .',\n"," '... naturaz gaindiko izaki gaiztoak existitzen direla itzalen mundu batean .\\thau ere esana dago harrezkero :',\n"," 'hau ere esana dago harrezkero :\\tantzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak , alegia .',\n"," 'antzinako karaktere errunikoen ahalmen magikoa erabiltzen duen gizonak ilunpeko ahalmen horiek inbokatu ditzake infernuko deabruak , alegia .\\tgizakia beti egon da izaki horien beldurrez .',\n"," 'gizakia beti egon da izaki horien beldurrez .\\tsorginkeria eta gaizkiaren gurtza existitzen dira gure egunetan ere .',\n"," 'sorginkeria eta gaizkiaren gurtza existitzen dira gure egunetan ere .\\tdeabruaren gaua',\n"," 'deabruaren gaua\\thamabost .',\n"," 'hamabost .\\tbi gehi hiru berdin bost .',\n"," 'bi gehi hiru berdin bost .\\tkarswell doktorea ?']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ylUjvCI3FxbF","executionInfo":{"status":"ok","timestamp":1617304492957,"user_tz":-120,"elapsed":86548,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["filename = \"./data/eu.tsv\"\n","save_text(filename, text)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQDfyBlnzSaQ"},"source":["## Vocabulary\n","\n","Sortu hiztegia BPE algoritmoa erabiliz (Sennrich et al., 2015).  Defektuz 10000 subtoken definituko ditugu. \n","\n","\n","*Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.(https://arxiv.org/abs/1508.07909). \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"le3gkSEXWobe","executionInfo":{"status":"ok","timestamp":1617304498067,"user_tz":-120,"elapsed":91649,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"641540c5-c69f-43f3-e1bb-37171ab9e53b"},"source":["!pip install tokenizers\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","def train_tokenizer(input_path, output_path, vocab_size=10000):\n","    tokenizer = ByteLevelBPETokenizer()\n","    tokenizer.train(files=[input_path], vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"<s>\", \"</s>\", \"<unk>\"])\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    tokenizer.save_model(output_path)\n","    return tokenizer\n","\n","def get_tokenizer(path):\n","    tokenizer = ByteLevelBPETokenizer(path + 'vocab.json', path + 'merges.txt')\n","    tokenizer._tokenizer.post_processor = BertProcessing(\n","        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","    )\n","    return tokenizer"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Collecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 8.0MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PGTLiG1iMdpl","executionInfo":{"status":"ok","timestamp":1617304537653,"user_tz":-120,"elapsed":131227,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["tokenizer_eu = train_tokenizer('./data/eu.tsv', './model/eu/')"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAPhlc5WzSaT","executionInfo":{"status":"ok","timestamp":1617304541998,"user_tz":-120,"elapsed":135560,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["tokenizer_en = train_tokenizer('./data/en_train.tsv', './model/en/')"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2-xPMxmzSab"},"source":["## Dialog Dataset\n","\n","Lehenik gure datuekin dataseta sortuko dugu \n"]},{"cell_type":"code","metadata":{"id":"VeZEfV_mXUoV","executionInfo":{"status":"ok","timestamp":1617304954633,"user_tz":-120,"elapsed":1306,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from torch.utils.data import Dataset\n","\n","class DialogDataset(Dataset):\n","\n","    def __init__(self, dataset_path, tokenizer):\n","\n","        self.tokenizer = tokenizer\n","\n","        self.examples = [(self.tokenizer.encode(line.strip().split('\\t')[0]).ids, \\\n","                          self.tokenizer.encode(line.strip().split('\\t')[1]).ids) \\\n","                         for line in open(dataset_path, 'r', encoding='utf-8').readlines()]\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i):\n","        return torch.tensor(self.examples[i][0]), torch.tensor(self.examples[i][1])"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_AbwsyxeBYf","executionInfo":{"status":"ok","timestamp":1617304948000,"user_tz":-120,"elapsed":1812,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from torch.utils.data import RandomSampler, DataLoader, random_split\n","import torch.nn as nn\n","\n","BATCH_SIZE = 64\n","MAX_LENGTH = 50\n","\n","def generate_batch(data_batch):\n","    src_batch, trg_batch = [], []\n","    for example in data_batch:\n","        src_batch.append(example[0][:MAX_LENGTH])\n","        trg_batch.append(example[1][:MAX_LENGTH])\n","    return nn.utils.rnn.pad_sequence(src_batch, tokenizer_eu.token_to_id('[PAD]')), \\\n","        nn.utils.rnn.pad_sequence(trg_batch, tokenizer_eu.token_to_id('[PAD]'))\n","\n","def get_dataloaders(filename, tokenizer):\n","    dataset = DialogDataset(filename, tokenizer)\n","    \n","    dataset_size = len(dataset)\n","    train_size = int(0.8 * dataset_size)\n","    val_size = int(0.1 * dataset_size)\n","    test_size = dataset_size - train_size - val_size\n","\n","    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","    train_sampler = RandomSampler(train_dataset)\n","    val_sampler = RandomSampler(val_dataset)\n","    test_sampler = RandomSampler(test_dataset)\n","    \n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n","\n","    return train_dataloader, val_dataloader, test_dataloader"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"k17BE4PedUIt","executionInfo":{"status":"ok","timestamp":1617304967381,"user_tz":-120,"elapsed":7953,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["train_dataloader, val_dataloader, test_dataloader = get_dataloaders('./data/en_train.tsv', tokenizer_en)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"McizbqJEkt2q","executionInfo":{"status":"ok","timestamp":1617305007081,"user_tz":-120,"elapsed":46932,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["train_dataloader, val_dataloader, test_dataloader = get_dataloaders('./data/eu.tsv', tokenizer_eu)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Drq-SOFZpAgx"},"source":["## Model\n","\n","Muturretik muturrerako sistema hau  Neural machine translation by jointly learning to align and translate (Bahdanau, D., Cho, K., & Bengio, Y. (2014)) artikuluan proposatutako sisteman oinarritzen da. Hau itzulpen automatikoko sistema bat da, beraz, dialogoa itzulpen automatikoko ataza bat bezala definitzen ari gara. Aukera hau ez da optimoa sinplifikazio handi bat baita, hala ere, esperimentu interesgarriak egiteko aukera ematen digu.\n","\n"]},{"cell_type":"code","metadata":{"id":"X_-VQxpZzSao","executionInfo":{"status":"ok","timestamp":1617305007087,"user_tz":-120,"elapsed":42852,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import random\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import Tensor\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: float):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self,\n","                src: Tensor) -> Tuple[Tensor]:\n","\n","        embedded = self.dropout(self.embedding(src))\n","\n","        outputs, hidden = self.rnn(embedded)\n","\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","        return outputs, hidden\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 attn_dim: int):\n","        super().__init__()\n","\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","\n","        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","\n","        self.attn = nn.Linear(self.attn_in, attn_dim)\n","\n","    def forward(self,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tensor:\n","\n","        src_len = encoder_outputs.shape[0]\n","\n","        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden,\n","            encoder_outputs),\n","            dim = 2)))\n","\n","        attention = torch.sum(energy, dim=2)\n","\n","        return F.softmax(attention, dim=1)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim: int,\n","                 emb_dim: int,\n","                 enc_hid_dim: int,\n","                 dec_hid_dim: int,\n","                 dropout: int,\n","                 attention: nn.Module):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","\n","        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def _weighted_encoder_rep(self,\n","                              decoder_hidden: Tensor,\n","                              encoder_outputs: Tensor) -> Tensor:\n","\n","        a = self.attention(decoder_hidden, encoder_outputs)\n","\n","        a = a.unsqueeze(1)\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n","\n","        return weighted_encoder_rep\n","\n","\n","    def forward(self,\n","                input: Tensor,\n","                decoder_hidden: Tensor,\n","                encoder_outputs: Tensor) -> Tuple[Tensor]:\n","\n","        input = input.unsqueeze(0)\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n","                                                          encoder_outputs)\n","\n","        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","\n","        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n","\n","        output = self.out(torch.cat((output,\n","                                     weighted_encoder_rep,\n","                                     embedded), dim = 1))\n","\n","        return output, decoder_hidden.squeeze(0)\n","\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder: nn.Module,\n","                 decoder: nn.Module,\n","                 device: torch.device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self,\n","                src: Tensor,\n","                trg: Tensor,\n","                teacher_forcing_ratio: float = 0.5) -> Tensor:\n","\n","        batch_size = src.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> token\n","        output = trg[0,:]\n","        for t in range(1, max_len):\n","            output, hidden = self.decoder(output, hidden, encoder_outputs)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            output = (trg[t] if teacher_force else top1)\n","\n","        return outputs"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pClkZpQXxKUL","executionInfo":{"status":"ok","timestamp":1617305007089,"user_tz":-120,"elapsed":36139,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"10995343-62e2-4c88-ddab-e2ef931f3997"},"source":["#Tamainak egokitu zuen beharretara\n","INPUT_DIM = 10000\n","OUTPUT_DIM = 10000\n","ENC_EMB_DIM = 512\n","DEC_EMB_DIM = 512\n","ENC_HID_DIM = 1024\n","DEC_HID_DIM = 1024\n","ATTN_DIM = 1024\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\"\"\"ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ATTN_DIM = 512\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\"\"\"\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","\n","def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","\n","model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","\n","def count_parameters(model: nn.Module):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","#Ignore the index of the padding\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_eu.token_to_id('[PAD]'))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["The model has 71,800,592 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BW1kpqVczSa2"},"source":["## Train\n","\n"]},{"cell_type":"code","metadata":{"id":"-OUZ5Gz7zSa4","executionInfo":{"status":"ok","timestamp":1617305007983,"user_tz":-120,"elapsed":857,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import math\n","import time\n","from tqdm import tqdm\n","\n","def train(model: nn.Module,\n","          train_dataloader: DataLoader,\n","          optimizer: optim.Optimizer,\n","          criterion: nn.Module,\n","          clip: float):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","    for iteration, (src, trg) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0, leave=True):\n","\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(train_dataloader)\n","\n","def valid(model: nn.Module, \n","             val_dataloader: DataLoader,\n","             criterion: nn.Module):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for iteration, (src, trg) in tqdm(enumerate(val_dataloader), total=len(val_dataloader), position=0, leave=True):\n","\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(val_dataloader)\n","\n","def epoch_time(start_time: int,\n","               end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yg70al43ztdD","colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"status":"error","timestamp":1617312478282,"user_tz":-120,"elapsed":7471132,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7e01b8c2-3900-4c7e-83e2-f96efd9d1025"},"source":["N_EPOCHS = 10\n","CLIP = 5.0\n","\n","best_valid_loss = float('inf')\n","for epoch in tqdm(range(N_EPOCHS), position=0, leave=True):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n","    valid_loss = valid(model, val_dataloader, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), './model/eu/model.pt')\n","    else:\n","        torch.save(model.state_dict(), './model/eu/model.pt')\n","\n","    print(f'\\nEpoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'Valid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["100%|██████████| 12745/12745 [1:58:44<00:00,  1.79it/s]\n","100%|██████████| 1594/1594 [05:03<00:00,  5.25it/s]\n","  0%|          | 0/12745 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 01 | Time: 123m 48s\n","Train Loss: 5.452 | Train PPL: 233.291\n","Valid Loss: 6.182 | Valid PPL: 483.762\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 71/12745 [00:36<1:56:03,  1.82it/s]"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-7b07d6a1eb4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-44149742c41e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"p3SoavU_8W5f"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"Qd-OsR4_8fBw","executionInfo":{"status":"ok","timestamp":1617312517880,"user_tz":-120,"elapsed":602,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import torch\n","import random\n","from argparse import ArgumentParser\n","\n","def decode(logits, decoding_strategy='max', k=3, temp=0.4):\n","    tokenizer.decode(logits.topk(10)[1][0].numpy())\n","    if decoding_strategy=='top1':\n","        target = logits.max(1)[1]\n","    elif decoding_strategy=='topk':\n","        target = logits.topk(k)[1][0][random.randint(0, k-1)].unsqueeze(-1)\n","    else:\n","        target = torch.multinomial(logits.squeeze().div(temp).exp().cpu(), 1)\n","    return target\n","\n","def evaluate(sentence):\n","    with torch.no_grad():\n","        target = torch.Tensor([tokenizer.token_to_id('<s>')]).long()\n","        output_sentence = []\n","        encoder_outputs, hidden = model.encoder(torch.Tensor(tokenizer.encode(sentence).ids).long().unsqueeze(-1))\n","        for t in range(MAX_LENGTH):\n","            # first input to the decoder is the <sos> token\n","            output, hidden = model.decoder(target, hidden, encoder_outputs)\n","            target = decode(output, decoding_strategy)\n","            if target.numpy() == tokenizer.token_to_id('</s>'):\n","                return tokenizer.decode(output_sentence)\n","            else:\n","                output_sentence.append(target.numpy()[0])\n","    return tokenizer.decode(output_sentence)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lbRHZ1F0H5R","executionInfo":{"status":"ok","timestamp":1617312732454,"user_tz":-120,"elapsed":1512,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["#Load model\n","#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device = 'cpu'\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","model = Seq2Seq(enc, dec, device).to(device)\n","model.load_state_dict(torch.load('./model/eu/model.pt', map_location=device))\n","\n","tokenizer = get_tokenizer('./model/eu/')"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0_O8MCwF4SO","executionInfo":{"status":"ok","timestamp":1617298924532,"user_tz":-120,"elapsed":37972,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"ed875320-964a-4b85-a12d-bdd1bb068d0f"},"source":["test_loss = valid(model, test_dataloader, criterion)\n","print(f'\\nTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"],"execution_count":296,"outputs":[{"output_type":"stream","text":["100%|██████████| 174/174 [00:37<00:00,  4.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Test Loss: 1.675 | Test PPL:   5.338\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"luI4WO-8z0cD"},"source":["MAX_LENGTH = 30\n","\n","#Print welcome message\n","print('-------------------------------')\n","print('Welcome to the Chit Chat system')\n","print(\"Write 'Bye' to end the system.\")\n","print('-------------------------------')\n","\n","#Main system loop\n","user = input('-')\n","model.eval()\n","decoding_strategy = \"top1\" # choices=['top1', 'topk', 'multinomial']\n","\n","while user != 'Bye' and user != 'Agur':\n","    sentence = evaluate(user)\n","    print('+' + sentence.capitalize())\n","    user = input('-')\n","    \n","sentence = evaluate(user)\n","print('+' + sentence.capitalize())"],"execution_count":null,"outputs":[]}]}